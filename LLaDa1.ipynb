{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71863ca-a558-4d73-bc97-abdb634ba5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a0816-b190-49ab-8917-56ecd37973d1",
   "metadata": {},
   "source": [
    "### CARGA Y PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3b521-1ef0-4ffe-a85e-4b35af595096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras filas del DataFrame de muestra:\n",
      "                                                    text Clase_Real  label\n",
      "21764  Never again...never again!!' This place is ter...         IA      0\n",
      "46722  put the carpet on the floor, they measure it, ...     Humano      1\n",
      "49245  [substeps] You may do this process before you ...     Humano      1\n",
      "30867  I believe mandatory minimum laws are unjust, c...     Humano      1\n",
      "10010  Wales coach Warren Gatland has hailed Shane Wi...         IA      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"yaful/MAGE\", split=\"test\")\n",
    "\n",
    "SAMPLE_SIZE =10000  # MAX 64000\n",
    "df_full = dataset.to_pandas()\n",
    "df_sample = df_full.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "df_sample['Clase_Real'] = df_sample['label'].apply(lambda x: 'Humano' if x == 1 else 'IA')\n",
    "\n",
    "print(\"\\nPrimeras filas del DataFrame de muestra:\")\n",
    "print(df_sample[['text', 'Clase_Real', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e3d9459-227c-4688-9fd7-16047586a53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Balance de Clases en la Muestra ---\n",
      "Clase_Real\n",
      "IA        50.09\n",
      "Humano    49.91\n"
     ]
    }
   ],
   "source": [
    "balance = df_sample['Clase_Real'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\n--- Balance de Clases en la Muestra ---\")\n",
    "print(balance.to_string())\n",
    "\n",
    "df_sample['Clase_Real_Binaria'] = df_sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "173dddf0-4d0c-4c91-b2b9-42bd4a3b8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['text_cleaned'] = df_sample['text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "df_sample = df_sample.dropna(subset=['text_cleaned'])\n",
    "\n",
    "df_sample.reset_index(drop=True, inplace=True)\n",
    "df_sample['Texto_ID'] = df_sample.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a820e8-cfd8-41e4-ab35-5c1be5fe4b77",
   "metadata": {},
   "source": [
    "### CARGA DE MODELOS Y CALCULO DE SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d6e3d-a605-4548-bcc8-3bcd155928aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "def resource_wrapper(fn, *args, device='cuda', verbose=True):\n",
    "\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss / 1024**3  # en GB\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = fn(*args)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[ERROR] La función {fn.__name__} falló: {e}\")\n",
    "        raise e\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # VRAM pico (solo si GPU)\n",
    "    vram_peak = torch.cuda.max_memory_allocated() / 1024**3 if device == 'cuda' else 0.0\n",
    "\n",
    "    # RAM usada\n",
    "    mem_after = process.memory_info().rss / 1024**3\n",
    "    cpu_mem = mem_after - mem_before\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[RESOURCE WRAPPER] Función: {fn.__name__}\")\n",
    "        print(f\"Tiempo: {elapsed:.2f}s | VRAM Pico: {vram_peak:.2f} GB | RAM usada: {cpu_mem:.2f} GB\")\n",
    "\n",
    "    return result, elapsed, vram_peak, cpu_mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e507e0-eb31-414e-8309-732ee3af7cd4",
   "metadata": {},
   "source": [
    "## SCORES DE LOS MODELOS\n",
    "$$\\text{Score}_{\\text{Secuencia}} = - \\frac{1}{N} \\sum_{i=1}^{N-1} \\text{Loss}(\\text{predicciones}_i, \\text{token real}_{i+1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ebc12-e00c-4e99-b523-7828a2ad4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando LLaDA-8B-Base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527e853612b347d7b89f605cdf5d09fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaDA cargado en: cuda:0\n",
      "\n",
      "Cargando GPT-2 Large (Proxy)...\n",
      "GPT cargado en: cuda\n",
      "\n",
      "Cargando LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0732cee10d4c14baad029ff014987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA cargado en: cuda:0\n",
      "\n",
      "Cargando BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT cargado en: cuda:0\n",
      "\n",
      "Cargando RoBERTa...\n",
      "RoBERTa cargado en: cuda:0\n",
      "\n",
      "Cargando GPT-3 Proxy (Neo)...\n",
      "GPT-3 Proxy cargado en: cuda:0\n",
      "\n",
      "Calculando Score_LLaDA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [26:50<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_llada_scores\n",
      "Tiempo: 1610.03s | VRAM Pico: 18.39 GB | RAM usada: 0.01 GB\n",
      "\n",
      "Calculando Score_GPT (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/4200728078.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [06:08<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_autoregressive_scores\n",
      "Tiempo: 368.03s | VRAM Pico: 17.85 GB | RAM usada: 0.01 GB\n",
      "\n",
      "Calculando Score_LLaMA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/4200728078.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [25:11<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_autoregressive_scores\n",
      "Tiempo: 1511.09s | VRAM Pico: 18.73 GB | RAM usada: 0.01 GB\n",
      "\n",
      "Calculando Score_GPT-3 (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/4200728078.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [12:55<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_autoregressive_scores\n",
      "Tiempo: 775.42s | VRAM Pico: 19.65 GB | RAM usada: 0.01 GB\n",
      "\n",
      "Calculando Score_BERT (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:21<00:00, 30.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_mlm_scores\n",
      "Tiempo: 81.84s | VRAM Pico: 16.79 GB | RAM usada: 0.01 GB\n",
      "\n",
      "Calculando Score_RoBERTa (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [01:20<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESOURCE WRAPPER] Función: batch_mlm_scores\n",
      "Tiempo: 80.28s | VRAM Pico: 17.32 GB | RAM usada: 0.01 GB\n",
      "\n",
      "--- Vista Previa de Scores Calculados ---\n",
      "   Texto_ID Clase_Real  Clase_Real_Binaria  Score_LLaDA  Score_GPT\n",
      "0         0         IA                   0    -8.229975  -8.639172\n",
      "1         1     Humano                   1    -3.688435  -8.639172\n",
      "2         2     Humano                   1    -7.827047  -8.639172\n",
      "3         3     Humano                   1   -12.513290  -8.639172\n",
      "4         4         IA                   0   -12.785653  -5.654218\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE = torch.bfloat16 if DEVICE == 'cuda' else torch.float32\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "LLADA_MODEL_NAME = 'GSAI-ML/LLaDA-8B-Base'\n",
    "GPT_MODEL_NAME = 'gpt2-large'\n",
    "LLAMA_MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "ROBERTA_MODEL_NAME = \"roberta-base\"\n",
    "GPT3_PROXY_MODEL_NAME = \"EleutherAI/gpt-neo-2.7B\"  \n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH = 512 \n",
    "BATCH_SIZE = 4           \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# a) LLaDA\n",
    "print(\"\\nCargando LLaDA-8B-Base...\")\n",
    "tokenizer_llada = AutoTokenizer.from_pretrained(LLADA_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "model_llada = AutoModel.from_pretrained(\n",
    "    LLADA_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "if hasattr(model_llada, \"tie_weights\"):\n",
    "    model_llada.tie_weights()\n",
    "\n",
    "LLADA_DEVICE = next(model_llada.parameters()).device\n",
    "print(\"LLaDA cargado en:\", LLADA_DEVICE)\n",
    "\n",
    "# b) GPT\n",
    "\n",
    "print(\"\\nCargando GPT-2 Large (Proxy)...\")\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(GPT_MODEL_NAME)\n",
    "\n",
    "if tokenizer_gpt.pad_token is None:\n",
    "    tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "\n",
    "model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "    GPT_MODEL_NAME,\n",
    "    dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "if tokenizer_gpt.pad_token_id >= model_gpt.config.vocab_size:\n",
    "    model_gpt.resize_token_embeddings(len(tokenizer_gpt))\n",
    "print(\"GPT cargado en:\", DEVICE)\n",
    "\n",
    "\n",
    "# c) LLaMA\n",
    "\n",
    "print(\"\\nCargando LLaMA...\")\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "LLAMA_DEVICE = next(model_llama.parameters()).device\n",
    "print(\"LLaMA cargado en:\", LLAMA_DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "# d) BERT\n",
    "\n",
    "print(\"\\nCargando BERT...\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "model_bert = AutoModelForMaskedLM.from_pretrained(\n",
    "    BERT_MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "BERT_DEVICE = next(model_bert.parameters()).device\n",
    "print(\"BERT cargado en:\", BERT_DEVICE)\n",
    "\n",
    "\n",
    "# e) RoBERTa\n",
    "\n",
    "print(\"\\nCargando RoBERTa...\")\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "\n",
    "model_roberta = AutoModelForMaskedLM.from_pretrained(\n",
    "    ROBERTA_MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "ROBERTA_DEVICE = next(model_roberta.parameters()).device\n",
    "print(\"RoBERTa cargado en:\", ROBERTA_DEVICE)\n",
    "\n",
    "\n",
    "# f) GPT-3\n",
    "\n",
    "print(\"\\nCargando GPT-3 Proxy (Neo)...\")\n",
    "tokenizer_gpt3 = AutoTokenizer.from_pretrained(GPT3_PROXY_MODEL_NAME)\n",
    "\n",
    "if tokenizer_gpt3.pad_token is None:\n",
    "    tokenizer_gpt3.pad_token = tokenizer_gpt3.eos_token\n",
    "\n",
    "model_gpt3 = AutoModelForCausalLM.from_pretrained(\n",
    "    GPT3_PROXY_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "if tokenizer_gpt3.pad_token_id >= model_gpt3.config.vocab_size:\n",
    "    model_gpt3.resize_token_embeddings(len(tokenizer_gpt3))\n",
    "\n",
    "GPT3_DEVICE = next(model_gpt3.parameters()).device\n",
    "print(\"GPT-3 Proxy cargado en:\", GPT3_DEVICE)\n",
    "\n",
    "\n",
    "def batch_llada_scores(texts, model, tokenizer, batch_size, device):\n",
    "    scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=DTYPE):\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs.logits[:, :-1, :]         # (B, T-1, V)\n",
    "            labels = inputs[\"input_ids\"][:, 1:]        # (B, T-1)\n",
    "\n",
    "            # Loss por TOKEN\n",
    "            loss_per_token = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                labels.reshape(-1),\n",
    "                reduction=\"none\"\n",
    "            ).view(labels.shape)                       # (B, T-1)\n",
    "\n",
    "            loss_per_sequence = loss_per_token.mean(dim=1)  # (B,)\n",
    "\n",
    "            scores.extend((-loss_per_sequence).cpu().tolist())\n",
    "\n",
    "    return scores\n",
    "\n",
    "def batch_autoregressive_scores(texts, model, tokenizer, batch_size, device):\n",
    "    scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            scores.extend((-loss.detach().cpu()).repeat(len(batch)).tolist())\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def batch_mlm_scores(texts, model, tokenizer, batch_size, device):\n",
    "    scores = []\n",
    "    vocab_size = model.config.vocab_size\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        input_ids = input_ids.clamp(0, vocab_size - 1)\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=DTYPE):\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            token_log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "            ll = token_log_probs.gather(\n",
    "                2, input_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            ll = (ll * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "\n",
    "            scores.extend(ll.cpu().tolist())\n",
    "\n",
    "    return scores\n",
    "\n",
    "texts = df_sample['text_cleaned']\n",
    "\n",
    "print(\"\\nCalculando Score_LLaDA (batch)...\")\n",
    "df_sample['Score_LLaDA'], t_llada, v_llada, ram_llada  = resource_wrapper(\n",
    "    batch_llada_scores, texts, model_llada, tokenizer_llada, BATCH_SIZE, LLADA_DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nCalculando Score_GPT (batch)...\")\n",
    "df_sample['Score_GPT'], t_gpt2, v_gpt2, ram_gtp2 = resource_wrapper(\n",
    "    batch_autoregressive_scores, texts, model_gpt, tokenizer_gpt, BATCH_SIZE, DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nCalculando Score_LLaMA (batch)...\")\n",
    "df_sample['Score_LLaMA'], t_llama, v_llama, ram_llama = resource_wrapper(\n",
    "    batch_autoregressive_scores, texts, model_llama, tokenizer_llama, BATCH_SIZE, LLAMA_DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nCalculando Score_GPT-3 (batch)...\")\n",
    "df_sample['Score_GPT3'], t_gpt3, v_gpt3, ram_gpt3 = resource_wrapper(\n",
    "    batch_autoregressive_scores, texts, model_gpt3, tokenizer_gpt3, BATCH_SIZE, GPT3_DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nCalculando Score_BERT (batch)...\")\n",
    "df_sample['Score_BERT'], t_bert, v_bert, ram_bert = resource_wrapper(\n",
    "    batch_mlm_scores, texts, model_bert, tokenizer_bert, BATCH_SIZE, BERT_DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\nCalculando Score_RoBERTa (batch)...\")\n",
    "df_sample['Score_RoBERTa'], t_roberta, v_roberta, ram_roberta = resource_wrapper(\n",
    "    batch_mlm_scores, texts, model_roberta, tokenizer_roberta, BATCH_SIZE, ROBERTA_DEVICE\n",
    ")\n",
    "\n",
    "df_final = df_sample.dropna(subset=['Score_LLaDA', 'Score_GPT'], how='all')\n",
    "\n",
    "print(\"\\n--- Vista Previa de Scores Calculados ---\")\n",
    "print(df_final[['Texto_ID', 'Clase_Real', 'Clase_Real_Binaria', 'Score_LLaDA', 'Score_GPT']].head())\n",
    "\n",
    "# Liberar memoria\n",
    "#del model_llada\n",
    "#del model_gpt\n",
    "#del model_llama\n",
    "#del model_bert\n",
    "#del model_roberta\n",
    "#del model_gpt3\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "df_final.to_csv('df_final_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba43395-8dbc-4390-a7db-179ebce240b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Modelo   Tiempo (s)  VRAM Pico (GB)  RAM Pico (GB)\n",
      "0        LLaDA  1610.034923       18.387353       0.007900\n",
      "1        GPT-2   368.034197       17.845390       0.010357\n",
      "2        LLaMA  1511.086120       18.729352       0.008083\n",
      "3  GPT-3 Proxy   775.420411       19.651498       0.008476\n",
      "4         BERT    81.844312       16.794811       0.005630\n",
      "5      RoBERTa    80.275546       17.322048       0.009373\n"
     ]
    }
   ],
   "source": [
    "resource_df = pd.DataFrame({\n",
    "    \"Modelo\": [\"LLaDA\", \"GPT-2\", \"LLaMA\", \"GPT-3 Proxy\", \"BERT\", \"RoBERTa\"],\n",
    "    \"Tiempo (s)\": [t_llada, t_gpt2, t_llama, t_gpt3, t_bert, t_roberta],\n",
    "    \"VRAM Pico (GB)\": [v_llada, v_gpt2, v_llama, v_gpt3, v_bert, v_roberta],\n",
    "    \"RAM Pico (GB)\": [ram_llada, ram_gtp2, ram_llama, ram_gpt3, ram_bert, ram_roberta]\n",
    "})\n",
    "\n",
    "print(resource_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee23150-244a-4196-9f50-d7fb10212daa",
   "metadata": {},
   "source": [
    "### BENCHMARK  Y COMPARACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45632ceb-9639-47f1-a1cc-9d017b4dc063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Procesando columna de score: Score_LLaDA ===\n",
      "LR AUC: 0.5403 | RF AUC: 0.5414 | XG AUC: 0.5787 | CAL AUC: 0.5787\n",
      "\n",
      "=== Procesando columna de score: Score_GPT ===\n",
      "LR AUC: 0.5161 | RF AUC: 0.4903 | XG AUC: 0.5060 | CAL AUC: 0.5052\n",
      "\n",
      "=== Procesando columna de score: Score_LLaMA ===\n",
      "LR AUC: 0.5108 | RF AUC: 0.4883 | XG AUC: 0.4904 | CAL AUC: 0.4994\n",
      "\n",
      "=== Procesando columna de score: Score_GPT3 ===\n",
      "LR AUC: 0.5145 | RF AUC: 0.4906 | XG AUC: 0.5004 | CAL AUC: 0.4960\n",
      "\n",
      "=== Procesando columna de score: Score_BERT ===\n",
      "LR AUC: 0.5314 | RF AUC: 0.5231 | XG AUC: 0.5489 | CAL AUC: 0.5445\n",
      "\n",
      "=== Procesando columna de score: Score_RoBERTa ===\n",
      "LR AUC: 0.6004 | RF AUC: 0.5320 | XG AUC: 0.5986 | CAL AUC: 0.6004\n",
      "\n",
      "\n",
      "===== COMPARATIVA FINAL =====\n",
      "\n",
      "       score_col  lr_roc_auc  lr_accuracy     lr_f1  lr_precision  lr_recall  \\\n",
      "0  Score_RoBERTa    0.600436       0.5823  0.504096      0.618496   0.425565   \n",
      "1    Score_LLaDA    0.540325       0.5340  0.518245      0.535319   0.502309   \n",
      "2     Score_BERT    0.531444       0.5097  0.412776      0.513082   0.345424   \n",
      "3      Score_GPT    0.516139       0.5135  0.513085      0.512608   0.513720   \n",
      "4     Score_GPT3    0.514525       0.5076  0.461291      0.508806   0.423172   \n",
      "5    Score_LLaMA    0.510768       0.5101  0.507968      0.509304   0.506709   \n",
      "\n",
      "   rf_roc_auc  rf_accuracy     rf_f1  rf_precision  ...  xg_roc_auc  \\\n",
      "0    0.532025       0.5215  0.519204      0.520788  ...    0.598612   \n",
      "1    0.541417       0.5214  0.518608      0.520733  ...    0.578721   \n",
      "2    0.523065       0.5110  0.512223      0.509963  ...    0.548899   \n",
      "3    0.490343       0.4939  0.493798      0.493020  ...    0.506016   \n",
      "4    0.490600       0.4933  0.491645      0.492327  ...    0.500367   \n",
      "5    0.488310       0.4913  0.493418      0.490492  ...    0.490431   \n",
      "\n",
      "   xg_accuracy     xg_f1  xg_precision  xg_recall  cal_roc_auc  cal_accuracy  \\\n",
      "0       0.5815  0.562826      0.587825   0.539971     0.600403        0.5858   \n",
      "1       0.5578  0.610889      0.544632   0.695850     0.578723        0.5597   \n",
      "2       0.5253  0.541362      0.522834   0.561807     0.544480        0.5230   \n",
      "3       0.5033  0.498831      0.502403   0.496698     0.505185        0.5046   \n",
      "4       0.4991  0.518049      0.498284   0.539569     0.495960        0.4996   \n",
      "5       0.4906  0.499535      0.489951   0.510127     0.499385        0.4965   \n",
      "\n",
      "     cal_f1  cal_precision  cal_recall  \n",
      "0  0.569224       0.591936    0.548390  \n",
      "1  0.644208       0.539848    0.799034  \n",
      "2  0.578997       0.517193    0.662989  \n",
      "3  0.509476       0.505926    0.534608  \n",
      "4  0.590398       0.499027    0.723496  \n",
      "5  0.472507       0.492818    0.481917  \n",
      "\n",
      "[6 rows x 21 columns]\n",
      "\n",
      "Resultados guardados en 'model_comparison_by_score_single_feature_ADVANCED.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RND = 42\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"df_final_scores.csv\",\n",
    "    sep=\",\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "score_cols = [\"Score_LLaDA\",\"Score_GPT\",\"Score_LLaMA\",\"Score_GPT3\",\"Score_BERT\",\"Score_RoBERTa\"]\n",
    "\n",
    "target = \"Clase_Real_Binaria\"\n",
    "\n",
    "df[target] = pd.to_numeric(df[target], errors=\"coerce\")\n",
    "\n",
    "for c in score_cols:\n",
    "    df[c] = pd.to_numeric(df[c].astype(str).str.replace(',','.'), errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[target] + score_cols)\n",
    "\n",
    "X = df[score_cols]\n",
    "y = df[target].astype(int)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "\n",
    "scoring = {\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"clf__n_estimators\": [100, 300],\n",
    "    \"clf__max_depth\": [None, 10, 30],\n",
    "    \"clf__min_samples_leaf\": [1, 3],\n",
    "}\n",
    "\n",
    "mlp_param_grid = {\n",
    "    \"clf__hidden_layer_sizes\": [(50,), (100,), (100,50)],\n",
    "    \"clf__alpha\": [1e-4, 1e-3],\n",
    "    \"clf__learning_rate_init\": [1e-3, 1e-4],\n",
    "    \"clf__max_iter\": [500],\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in score_cols:\n",
    "    print(f\"=== Procesando columna de score: {col} ===\")\n",
    "    Xi = df[[col]].values\n",
    "\n",
    "    # LOGISTIC REGRESSION\n",
    "    lr = LogisticRegression(max_iter=2000)\n",
    "    cv_res_lr = cross_validate(lr, Xi, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean_lr = {m: np.mean(cv_res_lr[f\"test_{m}\"]) for m in scoring.keys()}\n",
    "\n",
    "    # RANDOM FOREST\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=RND\n",
    "    )\n",
    "    cv_res_rf = cross_validate(rf, Xi, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean_rf = {m: np.mean(cv_res_rf[f\"test_{m}\"]) for m in scoring.keys()}\n",
    "\n",
    "    # XGBOOST\n",
    "    xg = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RND\n",
    "    )\n",
    "    cv_res_xg = cross_validate(xg, Xi, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean_xg = {m: np.mean(cv_res_xg[f\"test_{m}\"]) for m in scoring.keys()}\n",
    "\n",
    "    # XGBOOST CALIBRADO (PROBABILIDADES REALES)\n",
    "    cal = CalibratedClassifierCV(xg, method=\"isotonic\", cv=3)\n",
    "    cv_res_cal = cross_validate(cal, Xi, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean_cal = {m: np.mean(cv_res_cal[f\"test_{m}\"]) for m in scoring.keys()}\n",
    "\n",
    "    results.append({\n",
    "        \"score_col\": col,\n",
    "\n",
    "        \"lr_roc_auc\": mean_lr[\"roc_auc\"],\n",
    "        \"lr_accuracy\": mean_lr[\"accuracy\"],\n",
    "        \"lr_f1\": mean_lr[\"f1\"],\n",
    "        \"lr_precision\": mean_lr[\"precision\"],\n",
    "        \"lr_recall\": mean_lr[\"recall\"],\n",
    "\n",
    "        \"rf_roc_auc\": mean_rf[\"roc_auc\"],\n",
    "        \"rf_accuracy\": mean_rf[\"accuracy\"],\n",
    "        \"rf_f1\": mean_rf[\"f1\"],\n",
    "        \"rf_precision\": mean_rf[\"precision\"],\n",
    "        \"rf_recall\": mean_rf[\"recall\"],\n",
    "\n",
    "        \"xg_roc_auc\": mean_xg[\"roc_auc\"],\n",
    "        \"xg_accuracy\": mean_xg[\"accuracy\"],\n",
    "        \"xg_f1\": mean_xg[\"f1\"],\n",
    "        \"xg_precision\": mean_xg[\"precision\"],\n",
    "        \"xg_recall\": mean_xg[\"recall\"],\n",
    "\n",
    "        \"cal_roc_auc\": mean_cal[\"roc_auc\"],\n",
    "        \"cal_accuracy\": mean_cal[\"accuracy\"],\n",
    "        \"cal_f1\": mean_cal[\"f1\"],\n",
    "        \"cal_precision\": mean_cal[\"precision\"],\n",
    "        \"cal_recall\": mean_cal[\"recall\"],\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"LR AUC: {mean_lr['roc_auc']:.4f} | \"\n",
    "        f\"RF AUC: {mean_rf['roc_auc']:.4f} | \"\n",
    "        f\"XG AUC: {mean_xg['roc_auc']:.4f} | \"\n",
    "        f\"CAL AUC: {mean_cal['roc_auc']:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n===== COMPARATIVA FINAL =====\\n\")\n",
    "print(res_df.sort_values(by=\"xg_roc_auc\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "res_df.to_csv(\"model_comparison_by_score_single_feature_ADVANCED.csv\", index=False)\n",
    "print(\"\\nResultados guardados en 'model_comparison_by_score_single_feature_ADVANCED.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb178d3b-3ce3-45f1-b08e-411b751966f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:20:26.105566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 10:20:26.204711: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 10:20:27.722116: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_LLaDA\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765362028.050794    1154 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 818 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:15:00.0, compute capability: 8.6\n",
      "2025-12-10 10:20:31.735749: I external/local_xla/xla/service/service.cc:163] XLA service 0x7faf94010b70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-10 10:20:31.735778: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2025-12-10 10:20:31.816266: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-10 10:20:32.333848: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-12-10 10:20:32.629277: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-10 10:20:32.629388: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-10 10:20:32.629404: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-10 10:20:33.990964: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1139', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-10 10:20:34.870608: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1809', 552 bytes spill stores, 552 bytes spill loads\n",
      "\n",
      "2025-12-10 10:20:35.367302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1905', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1765362039.118694    6922 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-12-10 10:21:03.032530: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-10 10:21:03.959910: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_LLaDA): 0.5847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.41      0.48      1002\n",
      "           1       0.55      0.72      0.62       998\n",
      "\n",
      "    accuracy                           0.56      2000\n",
      "   macro avg       0.57      0.56      0.55      2000\n",
      "weighted avg       0.57      0.56      0.55      2000\n",
      "\n",
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_GPT\n",
      "==============================\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_GPT): 0.5030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.23      0.31      1002\n",
      "           1       0.50      0.77      0.60       998\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.50      0.50      0.46      2000\n",
      "weighted avg       0.50      0.50      0.46      2000\n",
      "\n",
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_LLaMA\n",
      "==============================\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_LLaMA): 0.5039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.25      0.33      1002\n",
      "           1       0.50      0.76      0.60       998\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.50      0.50      0.47      2000\n",
      "weighted avg       0.50      0.50      0.47      2000\n",
      "\n",
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_GPT3\n",
      "==============================\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_GPT3): 0.5340\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52      1002\n",
      "           1       0.52      0.52      0.52       998\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.52      0.52      0.52      2000\n",
      "\n",
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_BERT\n",
      "==============================\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_BERT): 0.5621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.52      0.53      1002\n",
      "           1       0.53      0.55      0.54       998\n",
      "\n",
      "    accuracy                           0.54      2000\n",
      "   macro avg       0.54      0.54      0.54      2000\n",
      "weighted avg       0.54      0.54      0.54      2000\n",
      "\n",
      "\n",
      "==============================\n",
      "Entrenando Deep MLP para: Score_RoBERTa\n",
      "==============================\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC (Score_RoBERTa): 0.6109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61      1002\n",
      "           1       0.60      0.55      0.57       998\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.60      0.59      0.59      2000\n",
      "weighted avg       0.60      0.59      0.59      2000\n",
      "\n",
      "\n",
      "===== COMPARATIVA FINAL DEEP MLP POR SCORE =====\n",
      "\n",
      "       score_col   roc_auc  accuracy                 model_path\n",
      "0  Score_RoBERTa  0.610874    0.5945  deep_mlp_Score_RoBERTa.h5\n",
      "1    Score_LLaDA  0.584680    0.5605    deep_mlp_Score_LLaDA.h5\n",
      "2     Score_BERT  0.562085    0.5365     deep_mlp_Score_BERT.h5\n",
      "3     Score_GPT3  0.533958    0.5205     deep_mlp_Score_GPT3.h5\n",
      "4    Score_LLaMA  0.503905    0.5025    deep_mlp_Score_LLaMA.h5\n",
      "5      Score_GPT  0.503026    0.4965      deep_mlp_Score_GPT.h5\n",
      "\n",
      "Resultados guardados en 'deep_mlp_by_score_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "score_cols = [\"Score_LLaDA\",\"Score_GPT\",\"Score_LLaMA\",\"Score_GPT3\",\"Score_BERT\",\"Score_RoBERTa\"]\n",
    "y = df[\"Clase_Real_Binaria\"].values\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in score_cols:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Entrenando Deep MLP para: {col}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    X = df[[col]].values   \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation=\"relu\", input_shape=(1,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_prob = model.predict(X_test).ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(f\"ROC AUC ({col}): {roc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    model_name = f\"deep_mlp_{col}.h5\"\n",
    "    model.save(model_name)\n",
    "\n",
    "    results.append({\n",
    "        \"score_col\": col,\n",
    "        \"roc_auc\": roc,\n",
    "        \"accuracy\": np.mean(y_pred == y_test),\n",
    "        \"model_path\": model_name\n",
    "    })\n",
    "\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(by=\"roc_auc\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n===== COMPARATIVA FINAL DEEP MLP POR SCORE =====\\n\")\n",
    "print(res_df)\n",
    "\n",
    "res_df.to_csv(\"deep_mlp_by_score_comparison.csv\", index=False)\n",
    "print(\"\\nResultados guardados en 'deep_mlp_by_score_comparison.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0a5dc-1571-4708-b3f5-235c140e4238",
   "metadata": {},
   "source": [
    "## Metricas del PAWN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17b3eb-1da9-4451-93a7-095f233e5c8c",
   "metadata": {},
   "source": [
    "\n",
    "### Fórmulas de las 5 Métricas de PAWN \n",
    "\n",
    "A continuación, $P_{i,j}$ representa la probabilidad asignada al *token* $j$ en la posición $i$ de la secuencia, $t_{i+1}$ es el *token* que ocurrió realmente en la posición $i+1$, $N$ es la longitud de la secuencia y $V$ es el tamaño del vocabulario.\n",
    "\n",
    "### 1. Log-Probability del Token Ocurrido\n",
    "Esta es la Log-Probabilidad asignada por el modelo al *token* real que aparece después de la posición $i$.\n",
    "\n",
    "$$M^{\\text{log-prob}}_i = \\log P_{i,t_{i+1}}$$\n",
    "\n",
    "### 2. Entropía de la Distribución\n",
    "Esta métrica mide la aleatoriedad (incertidumbre) de la predicción en la posición $i$.\n",
    "\n",
    "$$M^{\\text{entropy}}_i = -\\sum_{j=1}^{V} P_{i,j} \\log P_{i,j}$$\n",
    "\n",
    "### 3. Log-Probability Máxima\n",
    "Esta métrica mide la Log-Probabilidad del *token* más probable en el vocabulario en la posición $i$.\n",
    "\n",
    "$$M^{\\text{max-log-prob}}_i = \\max_{j=1,...,V} \\log P_{i,j}$$\n",
    "\n",
    "### 4. Rango del Token (Cuantil Normalizado)\n",
    "Esta métrica indica la posición ordenada del *token* real $t_{i+1}$ dentro de todas las opciones, normalizada por el tamaño del vocabulario $V$.\n",
    "\n",
    "$$M^{\\text{rank}}_i = \\frac{\\text{rank}(\\log P_{i,:,t_{i+1}})}{V}$$\n",
    "\n",
    "### 5. Suma de Probabilidades (Top-P Proxy)\n",
    "Esta métrica es la suma de las probabilidades de todos los *tokens* que son tan probables o más probables que el *token* real $t_{i+1}$.\n",
    "\n",
    "$$M^{\\text{top-p}}_i = \\sum_{j=1,...,V ; P_{i,j}\\geq P_{i,t_{i+1}}} P_{i,j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1307b0-71c3-4d02-84ea-e54b99c7bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_five_metrics(logits, labels, attention_mask):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas del paper a partir de logits y el siguiente token real (labels).\n",
    "\n",
    "    :param logits: Tensor de logits del modelo (B, T-1, V)\n",
    "    :param labels: Tensor de los tokens reales siguientes (B, T-1)\n",
    "    :param attention_mask: Tensor de la máscara de atención (B, T-1)\n",
    "    :return: 5 listas de métricas (una por cada métrica), promediadas por secuencia.\n",
    "    \"\"\"\n",
    "    B, T_minus_1, V = logits.shape\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=-1) # (B, T-1, V)\n",
    "    probs = torch.exp(log_probs)             # (B, T-1, V)\n",
    "\n",
    "    \n",
    "    log_prob_occured = log_probs.gather(\n",
    "        2, labels.unsqueeze(-1)\n",
    "    ).squeeze(-1) # (B, T-1)\n",
    "    \n",
    "    Mlog_prob = log_prob_occured\n",
    "\n",
    "    entropy = - (probs * log_probs).sum(dim=-1) # (B, T-1)\n",
    "    \n",
    "    Mentropy = entropy\n",
    "    \n",
    "    Mmax_log_prob, _ = log_probs.max(dim=-1) # (B, T-1)\n",
    "    \n",
    "\n",
    "    log_prob_occured_val = log_prob_occured.unsqueeze(-1) # (B, T-1, 1)\n",
    "\n",
    "    rank_mask = (log_probs >= log_prob_occured_val) \n",
    "    \n",
    "    Mrank = rank_mask.sum(dim=-1).float() / V # (B, T-1)\n",
    "\n",
    "    prob_occured = probs.gather(\n",
    "        2, labels.unsqueeze(-1)\n",
    "    ).squeeze(-1).unsqueeze(-1) # (B, T-1, 1)\n",
    "\n",
    "    top_p_mask = (probs >= prob_occured) # (B, T-1, V)\n",
    "\n",
    "    Mtop_p = (probs * top_p_mask).sum(dim=-1) # (B, T-1)\n",
    "    \n",
    "    masked_metrics = [Mlog_prob, Mentropy, Mmax_log_prob, Mrank, Mtop_p]\n",
    "    \n",
    "    results = []\n",
    "    sequence_lengths = attention_mask.sum(dim=1).float() # (B,)\n",
    "    \n",
    "    for M in masked_metrics:\n",
    "        # M: (B, T-1)\n",
    "        M_masked = M * attention_mask\n",
    "        M_sum_per_seq = M_masked.sum(dim=1) # (B,)\n",
    "        \n",
    "        # Promedio: Suma / Longitud\n",
    "        M_avg_per_seq = M_sum_per_seq / sequence_lengths\n",
    "        results.append(M_avg_per_seq.cpu().tolist())\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cfb58-ae45-4847-98ac-806a6aebae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_autoregressive_metrics(texts, model, tokenizer, batch_size, device):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas para modelos autoregresivos (LLaDA, GPT, LLaMA, GPT-3)\n",
    "    \"\"\"\n",
    "    all_metrics = [[], [], [], [], []]\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        inputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "            outputs = model(**inputs_on_device)\n",
    "            \n",
    "            logits = outputs.logits[:, :-1, :]        # (B, T-1, V)\n",
    "            labels = inputs_on_device[\"input_ids\"][:, 1:] # (B, T-1)\n",
    "\n",
    "            attention_mask = inputs_on_device[\"attention_mask\"][:, 1:] # (B, T-1)\n",
    "            \n",
    "            metrics = calculate_five_metrics(logits, labels, attention_mask)\n",
    "            \n",
    "            for j in range(5):\n",
    "                all_metrics[j].extend(metrics[j])\n",
    "                \n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aace0541-9193-4f23-b8b8-31353f7124aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_LLaDA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/485922399.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [28:04<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_GPT (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/485922399.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [06:39<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_LLaMA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/485922399.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [25:02<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_GPT-3 (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154/485922399.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 2500/2500 [13:35<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vista Previa de Scores Calculados ---\n",
      "   Texto_ID Clase_Real  Mlog_prob_LLaDA  Mentropy_LLaDA  Mlog_prob_GPT  \\\n",
      "0         0         IA        -8.698508        1.945712      -3.582509   \n",
      "1         1     Humano        -8.881931        1.905178      -3.740482   \n",
      "2         2     Humano       -11.092595        0.685308      -3.754062   \n",
      "3         3     Humano       -12.512227        0.530346      -2.835763   \n",
      "4         4         IA       -14.274117        0.359901      -1.785728   \n",
      "\n",
      "   Mentropy_GPT  \n",
      "0      3.662330  \n",
      "1      4.063519  \n",
      "2      3.667406  \n",
      "3      2.819254  \n",
      "4      2.196720  \n"
     ]
    }
   ],
   "source": [
    "metrics_names = ['Mlog_prob', 'Mentropy', 'Mmax_log_prob', 'Mrank', 'Mtop_p']\n",
    "\n",
    "# 1. LLaDA\n",
    "print(\"\\nCalculando 5 Métricas_LLaDA (batch)...\")\n",
    "results_llada = batch_autoregressive_metrics(texts, model_llada, tokenizer_llada, BATCH_SIZE, LLADA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_LLaDA'] = results_llada[i]\n",
    "\n",
    "\n",
    "# 2. GPT\n",
    "print(\"\\nCalculando 5 Métricas_GPT (batch)...\")\n",
    "results_gpt = batch_autoregressive_metrics(texts, model_gpt, tokenizer_gpt, BATCH_SIZE, DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_GPT'] = results_gpt[i]\n",
    "\n",
    "# 3. LLaMA\n",
    "print(\"\\nCalculando 5 Métricas_LLaMA (batch)...\")\n",
    "results_llama = batch_autoregressive_metrics(texts, model_llama, tokenizer_llama, BATCH_SIZE, LLAMA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_LLaMA'] = results_llama[i]\n",
    "\n",
    "# 4. GPT-3 Proxy\n",
    "print(\"\\nCalculando 5 Métricas_GPT-3 (batch)...\")\n",
    "results_gpt3 = batch_autoregressive_metrics(texts, model_gpt3, tokenizer_gpt3, BATCH_SIZE, GPT3_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_GPT3'] = results_gpt3[i]\n",
    "\n",
    "# 5. BERT\n",
    "print(\"\\nCalculando 5 Métricas_BERT (batch)...\")\n",
    "results_bert = batch_autoregressive_metrics(texts, model_bert, tokenizer_bert, BATCH_SIZE, BERT_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_BERT'] = results_bert[i]\n",
    "\n",
    "# 6. RoBERTa\n",
    "print(\"\\nCalculando 5 Métricas_RoBERTa (batch)...\")\n",
    "results_roberta = batch_autoregressive_metrics(texts, model_roberta, tokenizer_roberta, BATCH_SIZE, ROBERTA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_RoBERTa'] = results_roberta[i]\n",
    "\n",
    "\n",
    "df_final = df_sample.dropna(subset=['Mlog_prob_LLaDA', 'Mlog_prob_GPT'], how='all')\n",
    "\n",
    "print(\"\\n--- Vista Previa de Scores Calculados ---\")\n",
    "print(df_final[['Texto_ID', 'Clase_Real', 'Mlog_prob_LLaDA', 'Mentropy_LLaDA', 'Mlog_prob_GPT', 'Mentropy_GPT']].head())\n",
    "\n",
    "df_final.to_csv('df_final_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37b284-e918-4654-9a73-266b822a67a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLaDA] Epoch 1/200 loss=0.6781 test_auc=0.7030 f1=0.6879\n",
      "[LLaDA] Epoch 6/200 loss=0.6314 test_auc=0.7152 f1=0.7004\n",
      "[LLaDA] Epoch 11/200 loss=0.6209 test_auc=0.7196 f1=0.7025\n",
      "[LLaDA] Epoch 16/200 loss=0.6213 test_auc=0.7197 f1=0.6983\n",
      "[LLaDA] Epoch 21/200 loss=0.6159 test_auc=0.7225 f1=0.6982\n",
      "[LLaDA] Epoch 26/200 loss=0.6175 test_auc=0.7198 f1=0.7055\n",
      "[LLaDA] Epoch 31/200 loss=0.6142 test_auc=0.7226 f1=0.7038\n",
      "[LLaDA] Epoch 36/200 loss=0.6152 test_auc=0.7227 f1=0.7049\n",
      "[LLaDA] Epoch 41/200 loss=0.6123 test_auc=0.7217 f1=0.6951\n",
      "[LLaDA] Epoch 46/200 loss=0.6116 test_auc=0.7213 f1=0.7009\n",
      "[LLaDA] Epoch 51/200 loss=0.6100 test_auc=0.7232 f1=0.7024\n",
      "[LLaDA] Epoch 56/200 loss=0.6109 test_auc=0.7240 f1=0.7068\n",
      "[LLaDA] Epoch 61/200 loss=0.6125 test_auc=0.7241 f1=0.6962\n",
      "[LLaDA] Epoch 66/200 loss=0.6056 test_auc=0.7248 f1=0.7005\n",
      "[LLaDA] Epoch 71/200 loss=0.6071 test_auc=0.7230 f1=0.7010\n",
      "[LLaDA] Epoch 76/200 loss=0.6043 test_auc=0.7237 f1=0.6978\n",
      "[LLaDA] Epoch 81/200 loss=0.6029 test_auc=0.7252 f1=0.6965\n",
      "[LLaDA] Epoch 86/200 loss=0.6060 test_auc=0.7225 f1=0.7001\n",
      "[LLaDA] Early stopping en epoch 90. Mejor AUC val: 0.7255\n",
      "\n",
      ">>> RESULTADOS FINALES para LLaDA: {'accuracy': 0.667, 'precision': 0.6458699472759226, 'recall': 0.7364729458917836, 'f1': 0.6882022471910112, 'roc_auc': 0.7218048872195489}\n",
      "\n",
      "[GPT] Epoch 1/200 loss=0.6598 test_auc=0.7768 f1=0.7542\n",
      "[GPT] Epoch 6/200 loss=0.5837 test_auc=0.7985 f1=0.7579\n",
      "[GPT] Epoch 11/200 loss=0.5750 test_auc=0.8011 f1=0.7629\n",
      "[GPT] Epoch 16/200 loss=0.5638 test_auc=0.8023 f1=0.7655\n",
      "[GPT] Epoch 21/200 loss=0.5567 test_auc=0.8052 f1=0.7577\n",
      "[GPT] Epoch 26/200 loss=0.5570 test_auc=0.8053 f1=0.7639\n",
      "[GPT] Epoch 31/200 loss=0.5512 test_auc=0.8087 f1=0.7601\n",
      "[GPT] Epoch 36/200 loss=0.5508 test_auc=0.8089 f1=0.7602\n",
      "[GPT] Epoch 41/200 loss=0.5501 test_auc=0.8099 f1=0.7673\n",
      "[GPT] Epoch 46/200 loss=0.5488 test_auc=0.8108 f1=0.7647\n",
      "[GPT] Epoch 51/200 loss=0.5508 test_auc=0.8108 f1=0.7642\n",
      "[GPT] Epoch 56/200 loss=0.5489 test_auc=0.8089 f1=0.7604\n",
      "[GPT] Epoch 61/200 loss=0.5489 test_auc=0.8123 f1=0.7652\n",
      "[GPT] Epoch 66/200 loss=0.5453 test_auc=0.8120 f1=0.7655\n",
      "[GPT] Epoch 71/200 loss=0.5467 test_auc=0.8109 f1=0.7627\n",
      "[GPT] Epoch 76/200 loss=0.5442 test_auc=0.8119 f1=0.7703\n",
      "[GPT] Epoch 81/200 loss=0.5472 test_auc=0.8148 f1=0.7612\n",
      "[GPT] Epoch 86/200 loss=0.5428 test_auc=0.8152 f1=0.7639\n",
      "[GPT] Epoch 91/200 loss=0.5414 test_auc=0.8158 f1=0.7687\n",
      "[GPT] Epoch 96/200 loss=0.5424 test_auc=0.8155 f1=0.7699\n",
      "[GPT] Epoch 101/200 loss=0.5387 test_auc=0.8166 f1=0.7647\n",
      "[GPT] Epoch 106/200 loss=0.5356 test_auc=0.8163 f1=0.7609\n",
      "[GPT] Early stopping en epoch 110. Mejor AUC val: 0.8166\n",
      "\n",
      ">>> RESULTADOS FINALES para GPT: {'accuracy': 0.7445, 'precision': 0.6965294592413237, 'recall': 0.8647294589178357, 'f1': 0.7715690657130085, 'roc_auc': 0.8140972563890256}\n",
      "\n",
      "[LLaMA] Epoch 1/200 loss=0.6757 test_auc=0.7753 f1=0.7473\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_DIR = \"models_mlps\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"Clase_Real_Binaria\"  \n",
    "metrics_names = ['Mlog_prob', 'Mentropy', 'Mmax_log_prob', 'Mrank', 'Mtop_p']\n",
    "models_list = ['LLaDA', 'GPT', 'LLaMA', 'GPT3', 'BERT', 'RoBERTa']  \n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_dim,\n",
    "                 hidden_dims=(512, 256, 128, 64),\n",
    "                 dropout=0.4,\n",
    "                 batchnorm=True,\n",
    "                 activation=nn.ReLU,\n",
    "                 final_dropout=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        # final classifier\n",
    "        layers.append(nn.Linear(prev, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_metrics(y_true, y_pred_probs):\n",
    "    y_pred = (y_pred_probs[:,1] >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_probs[:,1])\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"df_final_metrics.csv\",\n",
    "    sep=\",\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise RuntimeError(f\"No encuentro la columna objetivo '{TARGET_COL}' en df_final_metrics.csv. Cambia TARGET_COL al nombre correcto.\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models_list:\n",
    "    cols = [f\"{m}_{model_name}\" for m in metrics_names]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Columns missing for {model_name}: {missing}. Saltando este modelo.\")\n",
    "        continue\n",
    "\n",
    "    subset = df[cols + [TARGET_COL]].dropna()\n",
    "    X = subset[cols].values\n",
    "    y = subset[TARGET_COL].values.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_ds = TabularDataset(X_train, y_train)\n",
    "    test_ds = TabularDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    if len(counts) == 1:\n",
    "        class_weights = torch.tensor([1.0, 1.0], dtype=torch.float32, device=DEVICE)\n",
    "    else:\n",
    "        inv = 1.0 / counts\n",
    "        weights = inv / inv.sum()\n",
    "        cw = np.zeros(2, dtype=np.float32)\n",
    "        for cls, w in zip(classes, weights):\n",
    "            cw[int(cls)] = w\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dims = (1024, 512, 256, 128)   # deep y ancho\n",
    "    model = DeepMLP(input_dim, hidden_dims=hidden_dims, dropout=0.4, batchnorm=True).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"test_auc\": []}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loss_epoch = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss.item() * xb.size(0)\n",
    "        loss_epoch /= len(train_loader.dataset)\n",
    "        history[\"train_loss\"].append(loss_epoch)\n",
    "\n",
    "        model.eval()\n",
    "        ys = []\n",
    "        yprobs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "                yprobs.append(probs)\n",
    "                ys.append(yb.numpy())\n",
    "        yprobs = np.vstack(yprobs)\n",
    "        ys = np.concatenate(ys)\n",
    "\n",
    "        metrics_eval = compute_metrics(ys, yprobs)\n",
    "        history[\"test_auc\"].append(metrics_eval[\"roc_auc\"])\n",
    "\n",
    "        # early stopping\n",
    "        if np.isfinite(metrics_eval[\"roc_auc\"]) and metrics_eval[\"roc_auc\"] > best_auc:\n",
    "            best_auc = metrics_eval[\"roc_auc\"]\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == EPOCHS-1:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{EPOCHS} loss={loss_epoch:.4f} test_auc={metrics_eval['roc_auc']:.4f} f1={metrics_eval['f1']:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"[{model_name}] Early stopping en epoch {epoch+1}. Mejor AUC val: {best_auc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    yprobs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            yprobs.append(probs)\n",
    "            ys.append(yb.numpy())\n",
    "    yprobs = np.vstack(yprobs)\n",
    "    ys = np.concatenate(ys)\n",
    "    final_metrics = compute_metrics(ys, yprobs)\n",
    "    print(f\"\\n>>> RESULTADOS FINALES para {model_name}: {final_metrics}\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"cols\": cols,\n",
    "        \"metrics\": final_metrics,\n",
    "        \"history\": history\n",
    "    }, os.path.join(MODEL_DIR, f\"mlp_{model_name}.pt\"))\n",
    "\n",
    "    results[model_name] = final_metrics\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"===== Resumen comparativo =====\")\n",
    "print(df_results)\n",
    "best_model = df_results['roc_auc'].idxmax()\n",
    "print(f\"\\nMejor modelo según ROC-AUC: {best_model} -> {df_results.loc[best_model].to_dict()}\")\n",
    "\n",
    "# guardado de resultados\n",
    "df_results.to_csv(os.path.join(MODEL_DIR, \"comparison_results.csv\"))\n",
    "print(f\"Modelos y resultados guardados en '{MODEL_DIR}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1a840-b254-443f-9fec-c96cd418fa68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
