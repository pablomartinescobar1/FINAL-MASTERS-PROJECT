{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c65bb7-fa87-40b9-9eb2-ddd0dc911f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Cambia el 1 por el id de la GPU que quieras usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb62d7e-35f3-40a4-866f-c2f268ef8a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1, 'NVIDIA RTX 4500 Ada Generation')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf3b9e5-6255-48b0-8c25-2c19abecaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yaful/MAGE\", split=\"test\")\n",
    "\n",
    "SAMPLE_SIZE =1000  # MAX 60000\n",
    "df_full = dataset.to_pandas()\n",
    "df_sample = df_full.sample(n=SAMPLE_SIZE, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26df22e-5962-41ce-9476-c3cd2e786d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras filas del DataFrame de muestra:\n",
      "                                                    text Clase_Real  label\n",
      "21764  Never again...never again!!' This place is ter...         IA      0\n",
      "46722  put the carpet on the floor, they measure it, ...     Humano      1\n",
      "49245  [substeps] You may do this process before you ...     Humano      1\n",
      "30867  I believe mandatory minimum laws are unjust, c...     Humano      1\n",
      "10010  Wales coach Warren Gatland has hailed Shane Wi...         IA      0\n"
     ]
    }
   ],
   "source": [
    "df_sample['Clase_Real'] = df_sample['label'].apply(lambda x: 'Humano' if x == 1 else 'IA')\n",
    "\n",
    "print(\"\\nPrimeras filas del DataFrame de muestra:\")\n",
    "print(df_sample[['text', 'Clase_Real', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55aaeb5-278f-41b6-9d87-5400f0ba38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['text_cleaned'] = df_sample['text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "df_sample = df_sample.dropna(subset=['text_cleaned'])\n",
    "\n",
    "df_sample.reset_index(drop=True, inplace=True)\n",
    "df_sample['Texto_ID'] = df_sample.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153d669f-80dc-4c7a-b0ae-d7151cf312da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in /home/jhuertas/.local/lib/python3.11/site-packages (0.49.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jhuertas/.local/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (24.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /opt/conda/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02453586-4c49-44d6-8b9c-9c52fe8aa6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando LLaDA-8B-Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 08:19:23.936912: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-08 08:19:24.020982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-08 08:19:25.880194: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f902a6049043a7bd95b6f0a01becd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaDA cargado en: cuda:0\n",
      "\n",
      "Cargando GPT-2 Large (Proxy)...\n",
      "GPT cargado en: cuda\n",
      "\n",
      "Cargando LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee594a04cabf494596e4a03f03c998d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA cargado en: cuda:0\n",
      "\n",
      "Cargando BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT cargado en: cuda:0\n",
      "\n",
      "Cargando RoBERTa...\n",
      "RoBERTa cargado en: cuda:0\n",
      "\n",
      "Cargando GPT-3 Proxy (Neo)...\n",
      "GPT-3 Proxy cargado en: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURACIÓN DE HARDWARE ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DTYPE = torch.bfloat16 if DEVICE == 'cuda' else torch.float32\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# --- MODELOS ---\n",
    "LLADA_MODEL_NAME = 'GSAI-ML/LLaDA-8B-Base'\n",
    "GPT_MODEL_NAME = 'gpt2-large'\n",
    "LLAMA_MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "ROBERTA_MODEL_NAME = \"roberta-base\"\n",
    "GPT3_PROXY_MODEL_NAME = \"EleutherAI/gpt-neo-2.7B\"  \n",
    "\n",
    "\n",
    "# --- INFERENCIA ---\n",
    "MAX_LENGTH = 512 \n",
    "BATCH_SIZE = 4           \n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 1. CARGA DE MODELOS\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# a) LLaDA\n",
    "print(\"\\nCargando LLaDA-8B-Base...\")\n",
    "tokenizer_llada = AutoTokenizer.from_pretrained(LLADA_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "model_llada = AutoModel.from_pretrained(\n",
    "    LLADA_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "if hasattr(model_llada, \"tie_weights\"):\n",
    "    model_llada.tie_weights()\n",
    "\n",
    "LLADA_DEVICE = next(model_llada.parameters()).device\n",
    "print(\"LLaDA cargado en:\", LLADA_DEVICE)\n",
    "\n",
    "# b) GPT\n",
    "\n",
    "print(\"\\nCargando GPT-2 Large (Proxy)...\")\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(GPT_MODEL_NAME)\n",
    "\n",
    "if tokenizer_gpt.pad_token is None:\n",
    "    tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "\n",
    "model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "    GPT_MODEL_NAME,\n",
    "    dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "if tokenizer_gpt.pad_token_id >= model_gpt.config.vocab_size:\n",
    "    model_gpt.resize_token_embeddings(len(tokenizer_gpt))\n",
    "print(\"GPT cargado en:\", DEVICE)\n",
    "\n",
    "\n",
    "# c) LLaMA\n",
    "\n",
    "print(\"\\nCargando LLaMA...\")\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "LLAMA_DEVICE = next(model_llama.parameters()).device\n",
    "print(\"LLaMA cargado en:\", LLAMA_DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "# d) BERT\n",
    "\n",
    "print(\"\\nCargando BERT...\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "model_bert = AutoModelForMaskedLM.from_pretrained(\n",
    "    BERT_MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "BERT_DEVICE = next(model_bert.parameters()).device\n",
    "print(\"BERT cargado en:\", BERT_DEVICE)\n",
    "\n",
    "\n",
    "# e) RoBERTa\n",
    "\n",
    "print(\"\\nCargando RoBERTa...\")\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "\n",
    "model_roberta = AutoModelForMaskedLM.from_pretrained(\n",
    "    ROBERTA_MODEL_NAME,\n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "ROBERTA_DEVICE = next(model_roberta.parameters()).device\n",
    "print(\"RoBERTa cargado en:\", ROBERTA_DEVICE)\n",
    "\n",
    "\n",
    "# f) GPT-3\n",
    "\n",
    "print(\"\\nCargando GPT-3 Proxy (Neo)...\")\n",
    "tokenizer_gpt3 = AutoTokenizer.from_pretrained(GPT3_PROXY_MODEL_NAME)\n",
    "\n",
    "if tokenizer_gpt3.pad_token is None:\n",
    "    tokenizer_gpt3.pad_token = tokenizer_gpt3.eos_token\n",
    "\n",
    "model_gpt3 = AutoModelForCausalLM.from_pretrained(\n",
    "    GPT3_PROXY_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=DTYPE\n",
    ").eval()\n",
    "\n",
    "if tokenizer_gpt3.pad_token_id >= model_gpt3.config.vocab_size:\n",
    "    model_gpt3.resize_token_embeddings(len(tokenizer_gpt3))\n",
    "\n",
    "GPT3_DEVICE = next(model_gpt3.parameters()).device\n",
    "print(\"GPT-3 Proxy cargado en:\", GPT3_DEVICE)\n",
    "\n",
    "\n",
    "texts = df_sample['text_cleaned']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b43acf2-df45-415b-a147-d9cbf56b039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_five_metrics(logits, labels, attention_mask):\n",
    "    B, T, V = logits.shape\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    probs = log_probs.exp()  # Calcular probs UNA VEZ\n",
    "    \n",
    "    # 1. Log-prob del token ocurrido\n",
    "    log_p = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    Mlog_prob = log_p\n",
    "    \n",
    "    # 2. Entropía\n",
    "    Mentropy = -(probs * log_probs).sum(dim=-1)\n",
    "    \n",
    "    # 3. Máxima log-prob\n",
    "    Mmax_log_prob, _ = log_probs.max(dim=-1)\n",
    "    \n",
    "    # 4. Rank normalizado (CORREGIDO)\n",
    "\n",
    "    rank = (log_probs > log_p.unsqueeze(-1)).sum(dim=-1).float() + 1.0\n",
    "    Mrank = rank / V\n",
    "    \n",
    "    # 5. Top-p (CORREGIDO)\n",
    "    prob_occured = probs.gather(2, labels.unsqueeze(-1)).squeeze(-1).unsqueeze(-1)\n",
    "    Mtop_p = (probs * (probs >= prob_occured)).sum(dim=-1)\n",
    "    \n",
    "    metrics = [Mlog_prob, Mentropy, Mmax_log_prob, Mrank, Mtop_p]\n",
    "    \n",
    "    seq_len = attention_mask.sum(dim=1).clamp(min=1)\n",
    "    results = []\n",
    "    for M in metrics:\n",
    "        results.append(((M * attention_mask).sum(dim=1) / seq_len).cpu().tolist())\n",
    "    \n",
    "    return results\n",
    "\n",
    "def batch_autoregressive_metrics(texts, model, tokenizer, batch_size, device):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas para modelos autoregresivos (LLaDA, GPT, LLaMA, GPT-3)\n",
    "    \"\"\"\n",
    "    all_metrics = [[], [], [], [], []] # 5 listas para las 5 métricas\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        inputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "            outputs = model(**inputs_on_device)\n",
    "            \n",
    "            logits = outputs.logits[:, :-1, :]        # (B, T-1, V)\n",
    "            labels = inputs_on_device[\"input_ids\"][:, 1:] # (B, T-1)\n",
    "\n",
    "            attention_mask = inputs_on_device[\"attention_mask\"][:, 1:] # (B, T-1)\n",
    "            \n",
    "            metrics = calculate_five_metrics(logits, labels, attention_mask)\n",
    "            \n",
    "            for j in range(5):\n",
    "                all_metrics[j].extend(metrics[j])\n",
    "                \n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa38992e-24ce-48ff-9f43-465279c46507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_five_metrics_mlm(logits, labels, attention_mask):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas PAWN para modelos MLM.\n",
    "    Para LLaDA en modo MLM: evalúa cada posición de la secuencia.\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    \n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # (B, T, V)\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # 1. Log-prob del token real\n",
    "    log_prob_occured = log_probs.gather(\n",
    "        2, labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T)\n",
    "    \n",
    "    Mlog_prob = log_prob_occured\n",
    "    \n",
    "    # 2. Entropía\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B, T)\n",
    "    Mentropy = entropy\n",
    "    \n",
    "    # 3. Max log-prob\n",
    "    Mmax_log_prob, _ = log_probs.max(dim=-1)  # (B, T)\n",
    "    \n",
    "    # 4. Rank (CORREGIDO - posición ordinal normalizada)\n",
    "    log_prob_occured_val = log_prob_occured.unsqueeze(-1)  # (B, T, 1)\n",
    "    rank = (log_probs > log_prob_occured_val).sum(dim=-1).float() + 1.0\n",
    "    Mrank = rank / V  # Normalizar\n",
    "    \n",
    "    # 5. Top-p\n",
    "    prob_occured = probs.gather(2, labels.unsqueeze(-1)).squeeze(-1).unsqueeze(-1)\n",
    "    Mtop_p = (probs * (probs >= prob_occured)).sum(dim=-1)\n",
    "    \n",
    "    masked_metrics = [Mlog_prob, Mentropy, Mmax_log_prob, Mrank, Mtop_p]\n",
    "    results = []\n",
    "    sequence_lengths = attention_mask.sum(dim=1).float().clamp(min=1)  # (B,)\n",
    "    \n",
    "    for M in masked_metrics:\n",
    "        M_masked = M * attention_mask\n",
    "        M_sum_per_seq = M_masked.sum(dim=1)\n",
    "        M_avg_per_seq = M_sum_per_seq / sequence_lengths\n",
    "        results.append(M_avg_per_seq.cpu().tolist())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def batch_mlm_metrics(texts, model, tokenizer, batch_size, device):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas PAWN para LLaDA en modo MLM.\n",
    "    LLaDA puede funcionar como MLM bidireccional.\n",
    "    \"\"\"\n",
    "    all_metrics = [[], [], [], [], []]\n",
    "    vocab_size = model.config.vocab_size\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        # CRÍTICO: Clamp de seguridad\n",
    "        input_ids = input_ids.clamp(0, vocab_size - 1)\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=DTYPE):\n",
    "            # LLaDA puede retornar logits para todas las posiciones\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Si LLaDA retorna logits de forma (B, T, V), úsalos directamente\n",
    "            logits = outputs.logits  # (B, T, V)\n",
    "            \n",
    "            # Calcular las 5 métricas\n",
    "            metrics = calculate_five_metrics_mlm(logits, input_ids, attention_mask)\n",
    "            \n",
    "            for j in range(5):\n",
    "                all_metrics[j].extend(metrics[j])\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22cd5c6-2be8-43ca-b4f6-d722fc59ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_five_metrics_diffusion(logits, labels, mask_positions):\n",
    "    \"\"\"\n",
    "    Calcula las 5 métricas PAWN solo en las posiciones ENMASCARADAS.\n",
    "    Basado en la lógica de reconstrucción de Language Diffusion.\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    \n",
    "    # Trabajamos con log_softmax para estabilidad numérica\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # 1. Log-probabilidad del token real (¿Qué tan bien reconstruye el modelo el texto original?)\n",
    "    log_p = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # 2. Entropía (Incertidumbre del modelo en las zonas enmascaradas)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    \n",
    "    # 3. Max log-prob (Confianza máxima en la predicción)\n",
    "    max_log_p, _ = log_probs.max(dim=-1)\n",
    "    \n",
    "    # 4. Rank normalizado (Posición ordinal del token real)\n",
    "    # Calculamos cuántos tokens tienen mayor probabilidad que el token real\n",
    "    rank = (log_probs > log_p.unsqueeze(-1)).sum(dim=-1).float() + 1.0\n",
    "    rank_norm = rank / V\n",
    "    \n",
    "    # 5. Top-p (Masa de probabilidad acumulada necesaria para llegar al token real)\n",
    "    # Indica qué tan \"esperada\" era la palabra en ese contexto\n",
    "    prob_occured = probs.gather(2, labels.unsqueeze(-1)).squeeze(-1).unsqueeze(-1)\n",
    "    top_p = (probs * (probs >= prob_occured)).sum(dim=-1)\n",
    "    \n",
    "    mask_float = mask_positions.float()\n",
    "    num_masked = mask_float.sum(dim=1).clamp(min=1)\n",
    "    \n",
    "    results = []\n",
    "    # Iteramos sobre las 5 métricas calculadas\n",
    "    for M in [log_p, entropy, max_log_p, rank_norm, top_p]:\n",
    "        # Filtramos para obtener el promedio SOLO de los tokens que fueron enmascarados\n",
    "        # Esto es lo que mide la capacidad de \"denoising\" o reconstrucción.\n",
    "        avg_val = (M * mask_float).sum(dim=1) / num_masked\n",
    "        results.append(avg_val.cpu().tolist())\n",
    "        \n",
    "    return results\n",
    "\n",
    "def batch_diffusion_metrics(texts, model, tokenizer, batch_size, device, mask_ratio=0.33, num_samples=10):\n",
    "    \"\"\"\n",
    "    Implementa el proceso de scoring por difusión para LLaDA.\n",
    "    Aumentamos el mask_ratio a 0.20 para forzar al modelo a usar más contexto.\n",
    "    Reducimos num_samples a 3 para balancear velocidad y estabilidad.\n",
    "    \"\"\"\n",
    "    all_metrics = [[] for _ in range(5)]\n",
    "    \n",
    "    # Identificar token de máscara correcto\n",
    "    if hasattr(tokenizer, 'mask_token_id') and tokenizer.mask_token_id is not None:\n",
    "        mask_id = tokenizer.mask_token_id\n",
    "    else:\n",
    "        # Fallback para modelos que no tienen [MASK] definido explícitamente\n",
    "        mask_id = tokenizer.vocab_size - 1 \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # Determinamos el tipo de dato para autocast (bfloat16 para GPUs modernas como RTX 4500 Ada)\n",
    "    dtype = torch.bfloat16 if device == 'cuda' else torch.float32\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"LLaDA Diffusion Scoring\"):\n",
    "        batch_texts = texts[i:i+batch_size].tolist()\n",
    "        # Acumulador para las muestras estocásticas de cada batch\n",
    "        batch_accum = [[] for _ in range(5)]\n",
    "        \n",
    "        # Realizamos varias pasadas con diferentes máscaras para obtener un promedio robusto (Monte Carlo)\n",
    "        for _ in range(num_samples):\n",
    "            inputs = tokenizer(\n",
    "                batch_texts, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            att_mask = inputs[\"attention_mask\"]\n",
    "            B, T = input_ids.shape\n",
    "            \n",
    "            # Generamos máscara aleatoria excluyendo el padding\n",
    "            mask_probs = torch.full((B, T), mask_ratio, device=device) * att_mask.float()\n",
    "            \n",
    "            # Opcional: Evitar enmascarar tokens especiales (si el tokenizer los tiene definidos)\n",
    "            if hasattr(tokenizer, 'all_special_ids'):\n",
    "                for special_id in tokenizer.all_special_ids:\n",
    "                    mask_probs[input_ids == special_id] = 0.0\n",
    "\n",
    "            mask_pos = torch.bernoulli(mask_probs).bool()\n",
    "            \n",
    "            # Garantizar que al menos un token esté enmascarado por secuencia\n",
    "            for j in range(B):\n",
    "                if not mask_pos[j].any():\n",
    "                    valid_indices = att_mask[j].nonzero(as_tuple=True)[0]\n",
    "                    if len(valid_indices) > 0:\n",
    "                        random_idx = valid_indices[torch.randint(0, len(valid_indices), (1,))]\n",
    "                        mask_pos[j, random_idx] = True\n",
    "\n",
    "            # Crear la versión \"corrupta\" del texto\n",
    "            corrupted_ids = input_ids.clone()\n",
    "            corrupted_ids[mask_pos] = mask_id\n",
    "            \n",
    "            with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=dtype):\n",
    "                # Predicción bidireccional: el modelo intenta adivinar los tokens en las máscaras\n",
    "                outputs = model(input_ids=corrupted_ids, attention_mask=att_mask)\n",
    "                \n",
    "                # Extraer métricas solo de las posiciones que el modelo tuvo que reconstruir\n",
    "                sample_m = calculate_five_metrics_diffusion(outputs.logits, input_ids, mask_pos)\n",
    "                \n",
    "                for m_idx in range(5):\n",
    "                    batch_accum[m_idx].append(sample_m[m_idx])\n",
    "        \n",
    "        # Promediar las muestras para reducir el ruido de la selección aleatoria de máscaras\n",
    "        for m_idx in range(5):\n",
    "            avg_res = np.array(batch_accum[m_idx]).mean(axis=0)\n",
    "            all_metrics[m_idx].extend(avg_res.tolist())\n",
    "            \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb781872-d6ee-4cef-9e8c-df4136a368cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_LLaDA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [38:37<00:00,  9.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_GPT (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183264/3737245445.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 250/250 [00:45<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_LLaMA (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183264/3737245445.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 250/250 [02:27<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_GPT-3 (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183264/3737245445.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "100%|██████████| 250/250 [01:29<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_BERT (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando 5 Métricas_RoBERTa (batch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:12<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vista Previa de Scores Calculados ---\n",
      "   Texto_ID Clase_Real  Mlog_prob_LLaDA  Mentropy_LLaDA  Mlog_prob_GPT  \\\n",
      "0         0         IA        -8.280295        6.899026      -3.580119   \n",
      "1         1     Humano        -6.892394        7.555557      -3.744196   \n",
      "2         2     Humano        -7.125575        7.838195      -3.750089   \n",
      "3         3     Humano        -7.458102        7.880184      -2.831601   \n",
      "4         4         IA        -7.435955        7.474788      -1.784918   \n",
      "\n",
      "   Mentropy_GPT  \n",
      "0      3.661952  \n",
      "1      4.065942  \n",
      "2      3.669372  \n",
      "3      2.818978  \n",
      "4      2.194524  \n"
     ]
    }
   ],
   "source": [
    "metrics_names = ['Mlog_prob', 'Mentropy', 'Mmax_log_prob', 'Mrank', 'Mtop_p']\n",
    "\n",
    "# 1. LLaDA\n",
    "print(\"\\nCalculando 5 Métricas_LLaDA (batch)...\")\n",
    "results_llada = batch_diffusion_metrics(texts, model_llada, tokenizer_llada, BATCH_SIZE, LLADA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_LLaDA'] = results_llada[i]\n",
    "\n",
    "\n",
    "# 2. GPT\n",
    "print(\"\\nCalculando 5 Métricas_GPT (batch)...\")\n",
    "results_gpt = batch_autoregressive_metrics(texts, model_gpt, tokenizer_gpt, BATCH_SIZE, DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_GPT'] = results_gpt[i]\n",
    "\n",
    "# 3. LLaMA\n",
    "print(\"\\nCalculando 5 Métricas_LLaMA (batch)...\")\n",
    "results_llama = batch_autoregressive_metrics(texts, model_llama, tokenizer_llama, BATCH_SIZE, LLAMA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_LLaMA'] = results_llama[i]\n",
    "\n",
    "# 4. GPT-3 Proxy\n",
    "print(\"\\nCalculando 5 Métricas_GPT-3 (batch)...\")\n",
    "results_gpt3 = batch_autoregressive_metrics(texts, model_gpt3, tokenizer_gpt3, BATCH_SIZE, GPT3_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_GPT3'] = results_gpt3[i]\n",
    "\n",
    "# 5. BERT\n",
    "print(\"\\nCalculando 5 Métricas_BERT (batch)...\")\n",
    "results_bert = batch_mlm_metrics(texts, model_bert, tokenizer_bert, BATCH_SIZE, BERT_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_BERT'] = results_bert[i]\n",
    "\n",
    "# 6. RoBERTa\n",
    "print(\"\\nCalculando 5 Métricas_RoBERTa (batch)...\")\n",
    "results_roberta = batch_mlm_metrics(texts, model_roberta, tokenizer_roberta, BATCH_SIZE, ROBERTA_DEVICE)\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    df_sample[f'{metric_name}_RoBERTa'] = results_roberta[i]\n",
    "\n",
    "\n",
    "df_final = df_sample.dropna(subset=['Mlog_prob_LLaDA', 'Mlog_prob_GPT'], how='all')\n",
    "\n",
    "print(\"\\n--- Vista Previa de Scores Calculados ---\")\n",
    "print(df_final[['Texto_ID', 'Clase_Real', 'Mlog_prob_LLaDA', 'Mentropy_LLaDA', 'Mlog_prob_GPT', 'Mentropy_GPT']].head())\n",
    "\n",
    "df_final.to_csv('df_final_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79898c46-31a4-4982-870f-e773980c9a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLaDA] Epoch 1/200 loss=0.7496 test_auc=0.5412 f1=0.0200\n",
      "[LLaDA] Epoch 6/200 loss=0.7386 test_auc=0.6101 f1=0.1552\n",
      "[LLaDA] Epoch 11/200 loss=0.7165 test_auc=0.6345 f1=0.3546\n",
      "[LLaDA] Epoch 16/200 loss=0.7002 test_auc=0.6524 f1=0.4189\n",
      "[LLaDA] Epoch 21/200 loss=0.7090 test_auc=0.6507 f1=0.4387\n",
      "[LLaDA] Epoch 26/200 loss=0.6956 test_auc=0.6514 f1=0.5537\n",
      "[LLaDA] Epoch 31/200 loss=0.6882 test_auc=0.6574 f1=0.5549\n",
      "[LLaDA] Epoch 36/200 loss=0.7035 test_auc=0.6574 f1=0.5934\n",
      "[LLaDA] Epoch 41/200 loss=0.7192 test_auc=0.6541 f1=0.6011\n",
      "[LLaDA] Epoch 46/200 loss=0.7242 test_auc=0.6567 f1=0.6162\n",
      "[LLaDA] Epoch 51/200 loss=0.7154 test_auc=0.6593 f1=0.6162\n",
      "[LLaDA] Epoch 56/200 loss=0.7010 test_auc=0.6565 f1=0.6096\n",
      "[LLaDA] Epoch 61/200 loss=0.6778 test_auc=0.6618 f1=0.5907\n",
      "[LLaDA] Early stopping en epoch 62. Mejor AUC val: 0.6638\n",
      "\n",
      ">>> RESULTADOS FINALES para LLaDA: {'accuracy': 0.61, 'precision': 0.6041666666666666, 'recall': 0.5918367346938775, 'f1': 0.5979381443298969, 'roc_auc': 0.6633653461384553}\n",
      "\n",
      "[GPT] Epoch 1/200 loss=0.7541 test_auc=0.6476 f1=0.6621\n",
      "[GPT] Epoch 6/200 loss=0.7220 test_auc=0.7092 f1=0.6996\n",
      "[GPT] Epoch 11/200 loss=0.7135 test_auc=0.7267 f1=0.7054\n",
      "[GPT] Epoch 16/200 loss=0.7101 test_auc=0.7279 f1=0.7132\n",
      "[GPT] Epoch 21/200 loss=0.6724 test_auc=0.7280 f1=0.7109\n",
      "[GPT] Epoch 26/200 loss=0.6623 test_auc=0.7259 f1=0.7137\n",
      "[GPT] Epoch 31/200 loss=0.6496 test_auc=0.7306 f1=0.7137\n",
      "[GPT] Epoch 36/200 loss=0.6535 test_auc=0.7318 f1=0.7188\n",
      "[GPT] Epoch 41/200 loss=0.6292 test_auc=0.7307 f1=0.7165\n",
      "[GPT] Epoch 46/200 loss=0.6375 test_auc=0.7324 f1=0.7194\n",
      "[GPT] Epoch 51/200 loss=0.6370 test_auc=0.7377 f1=0.7194\n",
      "[GPT] Epoch 56/200 loss=0.6369 test_auc=0.7314 f1=0.7222\n",
      "[GPT] Epoch 61/200 loss=0.6350 test_auc=0.7373 f1=0.7222\n",
      "[GPT] Epoch 66/200 loss=0.6138 test_auc=0.7362 f1=0.7273\n",
      "[GPT] Epoch 71/200 loss=0.6262 test_auc=0.7362 f1=0.7171\n",
      "[GPT] Epoch 76/200 loss=0.6175 test_auc=0.7403 f1=0.7222\n",
      "[GPT] Early stopping en epoch 78. Mejor AUC val: 0.7421\n",
      "\n",
      ">>> RESULTADOS FINALES para GPT: {'accuracy': 0.655, 'precision': 0.5935483870967742, 'recall': 0.9387755102040817, 'f1': 0.7272727272727273, 'roc_auc': 0.7316926770708283}\n",
      "\n",
      "[LLaMA] Epoch 1/200 loss=0.7487 test_auc=0.5989 f1=0.2459\n",
      "[LLaMA] Epoch 6/200 loss=0.7204 test_auc=0.7365 f1=0.6265\n",
      "[LLaMA] Epoch 11/200 loss=0.7006 test_auc=0.7489 f1=0.6792\n",
      "[LLaMA] Epoch 16/200 loss=0.6868 test_auc=0.7644 f1=0.7111\n",
      "[LLaMA] Epoch 21/200 loss=0.6776 test_auc=0.7617 f1=0.6991\n",
      "[LLaMA] Epoch 26/200 loss=0.6709 test_auc=0.7702 f1=0.7094\n",
      "[LLaMA] Epoch 31/200 loss=0.6738 test_auc=0.7681 f1=0.7179\n",
      "[LLaMA] Epoch 36/200 loss=0.6529 test_auc=0.7680 f1=0.7155\n",
      "[LLaMA] Epoch 41/200 loss=0.6618 test_auc=0.7704 f1=0.7265\n",
      "[LLaMA] Epoch 46/200 loss=0.6522 test_auc=0.7671 f1=0.7173\n",
      "[LLaMA] Epoch 51/200 loss=0.6523 test_auc=0.7705 f1=0.7280\n",
      "[LLaMA] Epoch 56/200 loss=0.6346 test_auc=0.7718 f1=0.7342\n",
      "[LLaMA] Epoch 61/200 loss=0.6404 test_auc=0.7699 f1=0.7319\n",
      "[LLaMA] Epoch 66/200 loss=0.6398 test_auc=0.7744 f1=0.7280\n",
      "[LLaMA] Early stopping en epoch 69. Mejor AUC val: 0.7744\n",
      "\n",
      ">>> RESULTADOS FINALES para LLaMA: {'accuracy': 0.67, 'precision': 0.6126760563380281, 'recall': 0.8877551020408163, 'f1': 0.725, 'roc_auc': 0.7677070828331333}\n",
      "\n",
      "[GPT3] Epoch 1/200 loss=0.7317 test_auc=0.6482 f1=0.5990\n",
      "[GPT3] Epoch 6/200 loss=0.7063 test_auc=0.6693 f1=0.7089\n",
      "[GPT3] Epoch 11/200 loss=0.6845 test_auc=0.6738 f1=0.7160\n",
      "[GPT3] Epoch 16/200 loss=0.6667 test_auc=0.6959 f1=0.7025\n",
      "[GPT3] Epoch 21/200 loss=0.6741 test_auc=0.7009 f1=0.7078\n",
      "[GPT3] Epoch 26/200 loss=0.6607 test_auc=0.7203 f1=0.7206\n",
      "[GPT3] Epoch 31/200 loss=0.6606 test_auc=0.7227 f1=0.7154\n",
      "[GPT3] Epoch 36/200 loss=0.6549 test_auc=0.7410 f1=0.7287\n",
      "[GPT3] Epoch 41/200 loss=0.6470 test_auc=0.7374 f1=0.7236\n",
      "[GPT3] Epoch 46/200 loss=0.6536 test_auc=0.7455 f1=0.7287\n",
      "[GPT3] Epoch 51/200 loss=0.6377 test_auc=0.7452 f1=0.7339\n",
      "[GPT3] Epoch 56/200 loss=0.6599 test_auc=0.7510 f1=0.7368\n",
      "[GPT3] Epoch 61/200 loss=0.6569 test_auc=0.7572 f1=0.7287\n",
      "[GPT3] Epoch 66/200 loss=0.6222 test_auc=0.7585 f1=0.7287\n",
      "[GPT3] Epoch 71/200 loss=0.6356 test_auc=0.7687 f1=0.7258\n",
      "[GPT3] Epoch 76/200 loss=0.6315 test_auc=0.7719 f1=0.7339\n",
      "[GPT3] Epoch 81/200 loss=0.6410 test_auc=0.7658 f1=0.7287\n",
      "[GPT3] Epoch 86/200 loss=0.6360 test_auc=0.7684 f1=0.7309\n",
      "[GPT3] Epoch 91/200 loss=0.6340 test_auc=0.7725 f1=0.7302\n",
      "[GPT3] Epoch 96/200 loss=0.6328 test_auc=0.7724 f1=0.7331\n",
      "[GPT3] Epoch 101/200 loss=0.6367 test_auc=0.7714 f1=0.7339\n",
      "[GPT3] Epoch 106/200 loss=0.6332 test_auc=0.7772 f1=0.7309\n",
      "[GPT3] Epoch 111/200 loss=0.6183 test_auc=0.7800 f1=0.7381\n",
      "[GPT3] Epoch 116/200 loss=0.6286 test_auc=0.7784 f1=0.7280\n",
      "[GPT3] Epoch 121/200 loss=0.6147 test_auc=0.7789 f1=0.7368\n",
      "[GPT3] Epoch 126/200 loss=0.6268 test_auc=0.7824 f1=0.7360\n",
      "[GPT3] Epoch 131/200 loss=0.6014 test_auc=0.7806 f1=0.7287\n",
      "[GPT3] Epoch 136/200 loss=0.6211 test_auc=0.7797 f1=0.7390\n",
      "[GPT3] Early stopping en epoch 139. Mejor AUC val: 0.7854\n",
      "\n",
      ">>> RESULTADOS FINALES para GPT3: {'accuracy': 0.67, 'precision': 0.6052631578947368, 'recall': 0.9387755102040817, 'f1': 0.736, 'roc_auc': 0.7842136854741897}\n",
      "\n",
      "[BERT] Epoch 1/200 loss=0.7571 test_auc=0.5159 f1=0.6505\n",
      "[BERT] Epoch 6/200 loss=0.7622 test_auc=0.5448 f1=0.6525\n",
      "[BERT] Epoch 11/200 loss=0.7386 test_auc=0.5494 f1=0.6738\n",
      "[BERT] Epoch 16/200 loss=0.7292 test_auc=0.5532 f1=0.6788\n",
      "[BERT] Epoch 21/200 loss=0.7265 test_auc=0.5601 f1=0.6740\n",
      "[BERT] Epoch 26/200 loss=0.7197 test_auc=0.5639 f1=0.6692\n",
      "[BERT] Epoch 31/200 loss=0.7191 test_auc=0.5680 f1=0.6692\n",
      "[BERT] Epoch 36/200 loss=0.6948 test_auc=0.5748 f1=0.6768\n",
      "[BERT] Epoch 41/200 loss=0.7019 test_auc=0.5714 f1=0.6566\n",
      "[BERT] Epoch 46/200 loss=0.7000 test_auc=0.5721 f1=0.6692\n",
      "[BERT] Epoch 51/200 loss=0.7133 test_auc=0.5769 f1=0.6641\n",
      "[BERT] Epoch 56/200 loss=0.7173 test_auc=0.5755 f1=0.6667\n",
      "[BERT] Epoch 61/200 loss=0.6854 test_auc=0.5800 f1=0.6641\n",
      "[BERT] Epoch 66/200 loss=0.6997 test_auc=0.5792 f1=0.6429\n",
      "[BERT] Epoch 71/200 loss=0.7060 test_auc=0.5819 f1=0.6482\n",
      "[BERT] Epoch 76/200 loss=0.7109 test_auc=0.5832 f1=0.6535\n",
      "[BERT] Epoch 81/200 loss=0.7079 test_auc=0.5865 f1=0.6457\n",
      "[BERT] Epoch 86/200 loss=0.6888 test_auc=0.5868 f1=0.6429\n",
      "[BERT] Epoch 91/200 loss=0.6927 test_auc=0.5908 f1=0.6320\n",
      "[BERT] Epoch 96/200 loss=0.7208 test_auc=0.5939 f1=0.6400\n",
      "[BERT] Epoch 101/200 loss=0.6889 test_auc=0.5963 f1=0.6478\n",
      "[BERT] Epoch 106/200 loss=0.7040 test_auc=0.5964 f1=0.6452\n",
      "[BERT] Epoch 111/200 loss=0.7058 test_auc=0.6015 f1=0.6426\n",
      "[BERT] Epoch 116/200 loss=0.6936 test_auc=0.6028 f1=0.6367\n",
      "[BERT] Epoch 121/200 loss=0.6959 test_auc=0.6051 f1=0.6423\n",
      "[BERT] Epoch 126/200 loss=0.6666 test_auc=0.5982 f1=0.6420\n",
      "[BERT] Epoch 131/200 loss=0.6932 test_auc=0.6047 f1=0.6452\n",
      "[BERT] Epoch 136/200 loss=0.6942 test_auc=0.6007 f1=0.6532\n",
      "[BERT] Epoch 141/200 loss=0.6945 test_auc=0.6026 f1=0.6393\n",
      "[BERT] Epoch 146/200 loss=0.6942 test_auc=0.6114 f1=0.6478\n",
      "[BERT] Epoch 151/200 loss=0.6769 test_auc=0.6063 f1=0.6532\n",
      "[BERT] Epoch 156/200 loss=0.7032 test_auc=0.6051 f1=0.6532\n",
      "[BERT] Epoch 161/200 loss=0.6874 test_auc=0.6133 f1=0.6531\n",
      "[BERT] Epoch 166/200 loss=0.6885 test_auc=0.6138 f1=0.6559\n",
      "[BERT] Epoch 171/200 loss=0.6848 test_auc=0.6078 f1=0.6502\n",
      "[BERT] Early stopping en epoch 174. Mejor AUC val: 0.6147\n",
      "\n",
      ">>> RESULTADOS FINALES para BERT: {'accuracy': 0.575, 'precision': 0.5436241610738255, 'recall': 0.826530612244898, 'f1': 0.6558704453441295, 'roc_auc': 0.6081432573029211}\n",
      "\n",
      "[RoBERTa] Epoch 1/200 loss=0.7459 test_auc=0.6446 f1=0.5667\n",
      "[RoBERTa] Epoch 6/200 loss=0.7451 test_auc=0.6462 f1=0.6020\n",
      "[RoBERTa] Epoch 11/200 loss=0.7135 test_auc=0.6525 f1=0.6000\n",
      "[RoBERTa] Epoch 16/200 loss=0.7083 test_auc=0.6423 f1=0.5990\n",
      "[RoBERTa] Epoch 21/200 loss=0.6847 test_auc=0.6407 f1=0.6146\n",
      "[RoBERTa] Epoch 26/200 loss=0.7085 test_auc=0.6392 f1=0.5900\n",
      "[RoBERTa] Epoch 31/200 loss=0.7019 test_auc=0.6466 f1=0.6042\n",
      "[RoBERTa] Early stopping en epoch 31. Mejor AUC val: 0.6525\n",
      "\n",
      ">>> RESULTADOS FINALES para RoBERTa: {'accuracy': 0.62, 'precision': 0.6170212765957447, 'recall': 0.5918367346938775, 'f1': 0.6041666666666666, 'roc_auc': 0.6465586234493798}\n",
      "\n",
      "===== Resumen comparativo =====\n",
      "         accuracy  precision    recall        f1   roc_auc\n",
      "LLaDA       0.610   0.604167  0.591837  0.597938  0.663365\n",
      "GPT         0.655   0.593548  0.938776  0.727273  0.731693\n",
      "LLaMA       0.670   0.612676  0.887755  0.725000  0.767707\n",
      "GPT3        0.670   0.605263  0.938776  0.736000  0.784214\n",
      "BERT        0.575   0.543624  0.826531  0.655870  0.608143\n",
      "RoBERTa     0.620   0.617021  0.591837  0.604167  0.646559\n",
      "\n",
      "Mejor modelo según ROC-AUC: GPT3 -> {'accuracy': 0.67, 'precision': 0.6052631578947368, 'recall': 0.9387755102040817, 'f1': 0.736, 'roc_auc': 0.7842136854741897}\n",
      "Modelos y resultados guardados en 'models_mlps/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_DIR = \"models_mlps\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"Clase_Real_Binaria\"  \n",
    "metrics_names = ['Mlog_prob', 'Mentropy', 'Mmax_log_prob', 'Mrank', 'Mtop_p']\n",
    "models_list = ['LLaDA', 'GPT', 'LLaMA', 'GPT3', 'BERT', 'RoBERTa']  \n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_dim,\n",
    "                 hidden_dims=(128, 64, 32),\n",
    "                 dropout=0.4,\n",
    "                 batchnorm=True,\n",
    "                 activation=nn.ReLU,\n",
    "                 final_dropout=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        # final classifier\n",
    "        layers.append(nn.Linear(prev, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_metrics(y_true, y_pred_probs):\n",
    "    y_pred = (y_pred_probs[:,1] >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_probs[:,1])\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"df_final_metrics.csv\",\n",
    "    sep=\",\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "df['Clase_Real_Binaria'] = df['label']\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise RuntimeError(f\"No encuentro la columna objetivo '{TARGET_COL}' en df_final_metrics.csv. Cambia TARGET_COL al nombre correcto.\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models_list:\n",
    "    cols = [f\"{m}_{model_name}\" for m in metrics_names]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Columns missing for {model_name}: {missing}. Saltando este modelo.\")\n",
    "        continue\n",
    "\n",
    "    subset = df[cols + [TARGET_COL]].dropna()\n",
    "    X = subset[cols].values\n",
    "    y = subset[TARGET_COL].values.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_ds = TabularDataset(X_train, y_train)\n",
    "    test_ds = TabularDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    if len(counts) == 1:\n",
    "        class_weights = torch.tensor([1.0, 1.0], dtype=torch.float32, device=DEVICE)\n",
    "    else:\n",
    "        inv = 1.0 / counts\n",
    "        weights = inv / inv.sum()\n",
    "        cw = np.zeros(2, dtype=np.float32)\n",
    "        for cls, w in zip(classes, weights):\n",
    "            cw[int(cls)] = w\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dims = (1024, 512, 256, 128)   # deep y ancho\n",
    "    model = DeepMLP(input_dim, hidden_dims=hidden_dims, dropout=0.4, batchnorm=True).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"test_auc\": []}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loss_epoch = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss.item() * xb.size(0)\n",
    "        loss_epoch /= len(train_loader.dataset)\n",
    "        history[\"train_loss\"].append(loss_epoch)\n",
    "\n",
    "        model.eval()\n",
    "        ys = []\n",
    "        yprobs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "                yprobs.append(probs)\n",
    "                ys.append(yb.numpy())\n",
    "        yprobs = np.vstack(yprobs)\n",
    "        ys = np.concatenate(ys)\n",
    "\n",
    "        metrics_eval = compute_metrics(ys, yprobs)\n",
    "        history[\"test_auc\"].append(metrics_eval[\"roc_auc\"])\n",
    "\n",
    "        # early stopping\n",
    "        if np.isfinite(metrics_eval[\"roc_auc\"]) and metrics_eval[\"roc_auc\"] > best_auc:\n",
    "            best_auc = metrics_eval[\"roc_auc\"]\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == EPOCHS-1:\n",
    "            print(f\"[{model_name}] Epoch {epoch+1}/{EPOCHS} loss={loss_epoch:.4f} test_auc={metrics_eval['roc_auc']:.4f} f1={metrics_eval['f1']:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"[{model_name}] Early stopping en epoch {epoch+1}. Mejor AUC val: {best_auc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    yprobs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            yprobs.append(probs)\n",
    "            ys.append(yb.numpy())\n",
    "    yprobs = np.vstack(yprobs)\n",
    "    ys = np.concatenate(ys)\n",
    "    final_metrics = compute_metrics(ys, yprobs)\n",
    "    print(f\"\\n>>> RESULTADOS FINALES para {model_name}: {final_metrics}\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"cols\": cols,\n",
    "        \"metrics\": final_metrics,\n",
    "        \"history\": history\n",
    "    }, os.path.join(MODEL_DIR, f\"mlp_{model_name}.pt\"))\n",
    "\n",
    "    results[model_name] = final_metrics\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"===== Resumen comparativo =====\")\n",
    "print(df_results)\n",
    "best_model = df_results['roc_auc'].idxmax()\n",
    "print(f\"\\nMejor modelo según ROC-AUC: {best_model} -> {df_results.loc[best_model].to_dict()}\")\n",
    "\n",
    "# guardado de resultados\n",
    "df_results.to_csv(os.path.join(MODEL_DIR, \"comparison_results.csv\"))\n",
    "print(f\"Modelos y resultados guardados en '{MODEL_DIR}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249e8b61-ad64-4c2b-935d-f8b030b5352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:47<00:00,  4.75s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:17<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:49<00:00,  4.76s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:18<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:49<00:00,  4.76s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:18<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:49<00:00,  4.76s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:18<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:48<00:00,  4.75s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:18<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:48<00:00,  4.75s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:16<00:00,  6.79s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:47<00:00,  4.75s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:15<00:00,  6.78s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [19:47<00:00,  4.75s/it]\n",
      "LLaDA Diffusion Scoring: 100%|██████████| 250/250 [28:16<00:00,  6.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GRID SEARCH LLaDA (SILENCIADO)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "mask_ratios = [ 0.25,0.275, 0.3,0.325, 0.35,0.375, 0.4,0.425]\n",
    "num_samples_list = [ 7,10]\n",
    "\n",
    "grid_results = []\n",
    "\n",
    "texts = df[\"text\"]\n",
    "labels = df[TARGET_COL].values\n",
    "\n",
    "for mask_ratio in mask_ratios:\n",
    "    for num_samples in num_samples_list:\n",
    "\n",
    "        llada_metrics = batch_diffusion_metrics(\n",
    "            texts=texts,\n",
    "            model=model_llada,\n",
    "            tokenizer=tokenizer_llada,\n",
    "            batch_size=4,\n",
    "            device=DEVICE,\n",
    "            mask_ratio=mask_ratio,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "\n",
    "        llada_df = pd.DataFrame(\n",
    "            np.column_stack(llada_metrics),\n",
    "            columns=[f\"{m}_LLaDA\" for m in metrics_names]\n",
    "        )\n",
    "        llada_df[TARGET_COL] = labels\n",
    "        llada_df.dropna(inplace=True)\n",
    "\n",
    "        X = llada_df.drop(columns=[TARGET_COL]).values\n",
    "        y = llada_df[TARGET_COL].values.astype(int)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            TabularDataset(X_train, y_train),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            TabularDataset(X_test, y_test),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        model_mlp = DeepMLP(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dims=(128, 64, 32),\n",
    "            dropout=0.4\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model_mlp.parameters(),\n",
    "            lr=LR,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_auc = -1\n",
    "        patience = 0\n",
    "\n",
    "        for _ in range(100):\n",
    "            model_mlp.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model_mlp(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model_mlp.eval()\n",
    "            ys, yprobs = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in test_loader:\n",
    "                    probs = torch.softmax(model_mlp(xb.to(DEVICE)), dim=-1)\n",
    "                    yprobs.append(probs[:,1].cpu())\n",
    "                    ys.append(yb)\n",
    "\n",
    "            auc = roc_auc_score(\n",
    "                torch.cat(ys).numpy(),\n",
    "                torch.cat(yprobs).numpy()\n",
    "            )\n",
    "\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= 10:\n",
    "                    break\n",
    "\n",
    "        grid_results.append({\n",
    "            \"mask_ratio\": mask_ratio,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"roc_auc\": best_auc\n",
    "        })\n",
    "\n",
    "        del model_mlp\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc697127-2c5a-46e2-a2b0-e3e22ed071f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mask_ratio  num_samples   roc_auc\n",
      "13       0.400           10  0.664366\n",
      "6        0.325            7  0.663365\n",
      "9        0.350           10  0.649960\n",
      "3        0.275           10  0.636555\n",
      "15       0.425           10  0.634754\n",
      "\n",
      "🏆 Mejores parámetros:\n",
      "mask_ratio      0.400000\n",
      "num_samples    10.000000\n",
      "roc_auc         0.664366\n",
      "Name: 13, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_grid = pd.DataFrame(grid_results).sort_values(\"roc_auc\", ascending=False)\n",
    "print(df_grid.head())\n",
    "print(\"\\n🏆 Mejores parámetros:\")\n",
    "print(df_grid.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad04ba34-c31b-405b-8120-f762d352a136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask_ratio</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.400</td>\n",
       "      <td>10</td>\n",
       "      <td>0.664366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.325</td>\n",
       "      <td>7</td>\n",
       "      <td>0.663365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.350</td>\n",
       "      <td>10</td>\n",
       "      <td>0.649960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275</td>\n",
       "      <td>10</td>\n",
       "      <td>0.636555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.425</td>\n",
       "      <td>10</td>\n",
       "      <td>0.634754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.350</td>\n",
       "      <td>7</td>\n",
       "      <td>0.626551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.375</td>\n",
       "      <td>10</td>\n",
       "      <td>0.626150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.300</td>\n",
       "      <td>7</td>\n",
       "      <td>0.616046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.325</td>\n",
       "      <td>10</td>\n",
       "      <td>0.615646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.601941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "      <td>10</td>\n",
       "      <td>0.597439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.300</td>\n",
       "      <td>10</td>\n",
       "      <td>0.574130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.425</td>\n",
       "      <td>7</td>\n",
       "      <td>0.553021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275</td>\n",
       "      <td>7</td>\n",
       "      <td>0.549520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.375</td>\n",
       "      <td>7</td>\n",
       "      <td>0.547019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.526411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mask_ratio  num_samples   roc_auc\n",
       "13       0.400           10  0.664366\n",
       "6        0.325            7  0.663365\n",
       "9        0.350           10  0.649960\n",
       "3        0.275           10  0.636555\n",
       "15       0.425           10  0.634754\n",
       "8        0.350            7  0.626551\n",
       "11       0.375           10  0.626150\n",
       "4        0.300            7  0.616046\n",
       "7        0.325           10  0.615646\n",
       "12       0.400            7  0.601941\n",
       "1        0.250           10  0.597439\n",
       "5        0.300           10  0.574130\n",
       "14       0.425            7  0.553021\n",
       "2        0.275            7  0.549520\n",
       "10       0.375            7  0.547019\n",
       "0        0.250            7  0.526411"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af759ba0-2fd8-4a24-8013-f497d6d11dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
